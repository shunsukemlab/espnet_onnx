{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shunsukemlab/espnet_onnx/blob/master/demo/simple_asr_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ6qvwFeucCJ"
      },
      "source": [
        "# espnet_onnx demonstration\n",
        "\n",
        "This notebook provides a simple demonstration of how to export your trained model and use it for inference.\n",
        "\n",
        "see also:\n",
        "- ESPnet: https://github.com/espnet/espnet\n",
        "- espnet_onnx: https://github.com/Masao-Someki/espnet_onnx\n",
        "\n",
        "Author: [Masao Someki](https://github.com/Masao-Someki)\n",
        "\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- Install Dependency\n",
        "- Export your model\n",
        "- Inference with onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rFOApIAucCP"
      },
      "source": [
        "# Install Dependency\n",
        "To run this demo, you need to install the following packages.\n",
        "- espnet_onnx\n",
        "- torch >= 1.11.0 (already installed in Colab)\n",
        "- espnet\n",
        "- espnet_model_zoo\n",
        "- onnx\n",
        "\n",
        "`torch`, `espnet`, `espnet_model_zoo`, `onnx` is required to run the exportation demo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pythonとtorchをインストールしなおす"
      ],
      "metadata": {
        "id": "yiaDM5n9FHIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install python3.8\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
        "!python --version\n",
        "!apt-get install python3.8-distutils\n",
        "!apt-get install python3-pip"
      ],
      "metadata": {
        "id": "tkZ8XfNUFGnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.pytorch.org/whl/cpu/torch-2.0.1%2Bcpu-cp38-cp38-linux_x86_64.whl\n",
        "!pip install torch-2.0.1%2Bcpu-cp38-cp38-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "q1cduK-uF0bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsOAQMqiucCS"
      },
      "outputs": [],
      "source": [
        "!pip install -U espnet_onnx espnet espnet_model_zoo onnx --no-cache-dir\n",
        "\n",
        "# in this demo, we need to update scipy to avoid an error\n",
        "!pip install -U scipy numpy==1.23.5 pyworld==0.3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goQZprrKucCW"
      },
      "source": [
        "And we need additional dependency `onnxruntime-gpu` to run inference on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTbHWO7wucCX"
      },
      "outputs": [],
      "source": [
        "!pip install onnxruntime==1.14.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/espnet/espnet_onnx/releases/download/custom_ort_v1.14.1.espnet/onnxruntime-1.14.1.espnet-cp38-cp38-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "ZPx-U--FEGfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv onnxruntime-1.14.1.espnet-cp38-cp38-linux_x86_64.whl onnxruntime-1.14.1-cp38-cp38-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "p_qfmJmDFbbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ./onnxruntime-1.14.1-cp38-cp38-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "HVY8reKfEH8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.1 --upgrade"
      ],
      "metadata": {
        "id": "N6aimRv5RNIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio==2.0.1"
      ],
      "metadata": {
        "id": "5Qx8z9MIWobb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VszOmVZlucCY"
      },
      "source": [
        "# Export your model\n",
        "\n",
        "## Export model from espnet_model_zoo\n",
        "\n",
        "The easiest way to export a model is to use `espnet_model_zoo`. You can download, unpack, and export the pretrained models with `export_from_pretrained` method.\n",
        "`espnet_onnx` will save the onnx models into cache directory, which is `${HOME}/.cache/espnet_onnx` in default."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/espnet/espnet_onnx/master/espnet_onnx/export/convert_map.yml -O /usr/local/lib/python3.10/dist-packages/espnet_onnx/export/convert_map.yml\n"
      ],
      "metadata": {
        "id": "3nbVT-0NLmhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wQBREknucCZ"
      },
      "outputs": [],
      "source": [
        "# export the model.\n",
        "from espnet_onnx.export import ASRModelExport\n",
        "\n",
        "tag_name = 'reazon-research/reazonspeech-espnet-v2'\n",
        "\n",
        "m = ASRModelExport()\n",
        "m.set_export_config(\n",
        "    max_seq_len=5000,\n",
        ")\n",
        "m.export_from_pretrained(\n",
        "    tag_name,  optimize=True,\n",
        "  quantize=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaMxMeewucCa"
      },
      "source": [
        "# Inference with onnxruntime\n",
        "Now, let's use the exported models for inference.\n",
        "Please enable the GPU resource to run the following codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgS9a0WnucCb"
      },
      "outputs": [],
      "source": [
        "# please provide the tag_name to specify exported model.\n",
        "tag_name = 'reazon-research/reazonspeech-espnet-v2'\n",
        "export_dir = f'/root/.cache/espnet_onnx/{tag_name}'\n",
        "# upload wav file and let's inference!\n",
        "import librosa\n",
        "from google.colab import files\n",
        "\n",
        "wav_file = files.upload()\n",
        "y, sr = librosa.load(list(wav_file.keys())[0], sr=16000)\n",
        "\n",
        "# Use the exported onnx file to inference.\n",
        "from espnet_onnx import Speech2Text\n",
        "\n",
        "speech2text = Speech2Text(model_dir=export_dir, providers=['CPUExecutionProvider'])\n",
        "nbest = speech2text(y)\n",
        "print(nbest[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-2HzvJMBQBTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/espnet_onnx/reazon-research/reazonspeech-espnet-v2/"
      ],
      "metadata": {
        "id": "msxX3HR_Pay6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "i6O7sr3OPl0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}